{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### EEE3314-02: Introduction to Artificial Intelligence\n",
    "\n",
    "# Assignment \\# III: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> **Due date: Nov. 2, 2018.** </div> \n",
    "<div style=\"text-align: right\"> **Please upload your file @ yscec by 9 PM.** </div> \n",
    "<h4><div style=\"text-align: right\"> **For one day late (70% credit), please send your file to your TAs in the form of [ID_Name_HW3.ipynb].**</div> </h4>\n",
    "<div style=\"text-align: right\"> Beomjun Kim(김범준): <a href=\"mailto:beomjun.kim@yonsei.ac.kr\">beomjun.kim@yonsei.ac.kr</a> </div>\n",
    "<div style=\"text-align: right\"> Chanho Um(엄찬호): <a href=\"mailto:chanho0103@yonsei.ac.kr\">chanho0103@yonsei.ac.kr</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assignement Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <h4><span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically. **</span> </h4>\n",
    "\n",
    "### *Collaboration policy:*\n",
    "- You may discuss the questions.   \n",
    "- Each student writes their own answers.   \n",
    "- **Write on your homework anyone with whom you collaborate.**   \n",
    "- Each student must write their own code for the programming part (**if not you will get a F grade**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:blue\">[2016142072] [NamGilHyun]</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Stochastic Gradient Descent (SGD) to optimize a logistic regression model to predict whether a given patient have diabetes or not. In clinical informatics, machine learning approahces have been widely adopted to predict clinically adverse events based on patient data. For this problem, we will use the Pima Indians Diabetes Data Set. The data on each patient include:\n",
    "\n",
    "- $\\texttt{label}$: The output class variable (0 - normal or 1 - diabetes)  \n",
    "- $\\texttt{num_preg}$: Number of times pregnant \n",
    "- $\\texttt{PGC}$: Plasma glucose concentration at 2 hours in an oral glucose tolerance test (PGC)\n",
    "- $\\texttt{DBP}$: Diastolic blood pressure (DBP)\n",
    "- $\\texttt{tricept}$: Triceps skin fold thickness (tricept, unit: mm)  \n",
    "- $\\texttt{insulin}$: 2-Hour serum insulin (insulin, unit: μU/ml)\n",
    "- $\\texttt{BMI}$: Body mass index (BMI)\n",
    "- $\\texttt{ped_func}$: Diabetes pedigree function\n",
    "- $\\texttt{age}$: Age (age, years)\n",
    "\n",
    "Among all 768 patients, we will separate 500 patients as training data ($\\texttt{logistic_regression_training_data.csv}$) and 268 patients as test data ($\\texttt{logistic_regression_test_data.csv}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Logistic regression method using SGD\n",
    "(*50 points*) P1.1 Implement a logistic regression method using SGD. You may assume that the data is randomly ordered. \n",
    "\n",
    "(*10 points*) P1.2 Plot $\\bar L$ every 100 iterations, e.g., [100, 200, 300,...], defined as\n",
    "\n",
    "$\\begin{equation} \\bar L(T) = \\frac{1}{T}\\sum_{t=1}^{T} (\\hat y^t - y^t)^2 \\end{equation}$,   \n",
    "where $T$ is the number of iterations and $\\hat y^t$ (either 0 or 1) is the predicted label for sample $\\bf{x}^t$ using the weights $\\bf{w}^{t-1}$. \n",
    "\n",
    "> Initialize the weight vector $w$ and the bias $w_0$ to 0.    \n",
    "> Learning rate = 0.8.    \n",
    "> Number of iterations = 100,000.\n",
    "\n",
    "(*10 points*) P1.3 Use the model weights to predict whether each patient in the test set has diabetes, for every 100 steps. Plot sum of squared errors of your prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model evaluation and feature interpretations \n",
    "(*10 points*) P2.1 Using the model after 100,000 iterations, measure test accuracy. Accuracy is the fraction of predictions our model got right. That is, it is computed by Number of correct predictions / Total number of predictions.\n",
    "\n",
    "(*10 points*) P2.2 SGD oscillates around a solution (noisy convergence). In order to minimize risk of picking bad weights, we typically use an average model. Measure test accuracy again, using an average model over the weights for the last 1000 iterations\n",
    "\n",
    "(*10 points*) P2.3 Report the weights of following features, $\\texttt{BMI}$, $\\texttt{insulin}$, and $\\texttt{PGC}$, and provide an interpretation of the effect of these features for diabetes classification based on the inferred weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>num_preg</th>\n",
       "      <th>PGC</th>\n",
       "      <th>DBP</th>\n",
       "      <th>tricept</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ped_func</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.758794</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.187020</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.663317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.095645</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.467337</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.335320</td>\n",
       "      <td>0.144748</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.532042</td>\n",
       "      <td>0.076857</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  num_preg       PGC       DBP   tricept  insulin       BMI  ped_func  \\\n",
       "0      1  0.470588  0.758794  0.639344  0.507937     0.35  0.639344  0.187020   \n",
       "1      1  0.235294  0.663317  0.000000  0.000000     0.00  0.490313  0.095645   \n",
       "2      0  0.058824  0.467337  0.459016  0.174603     0.00  0.335320  0.144748   \n",
       "3      0  0.058824  0.547739  0.311475  0.285714     0.20  0.344262  0.140478   \n",
       "4      1  0.176471  0.869347  0.688525  0.523810     0.79  0.532042  0.076857   \n",
       "\n",
       "        age  \n",
       "0  0.250000  \n",
       "1  0.033333  \n",
       "2  0.016667  \n",
       "3  0.083333  \n",
       "4  0.016667  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# Data load\n",
    "data = pd.read_csv('logistic_regression_training_data.csv') \n",
    "test_data = pd.read_csv('logistic_regression_testing_data.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe객체의 head함수를 이용해서 처음 5개의 데이터들을 살펴보았다. 그 결과 총 8개의 feature가 있고, feature value들은 0부터 1사이의 값으로 정규화되어있음을 확인할 수 있었다. 또한 위의 문제 설명에 나와있다시피 label 이 0이면 정상, label이 1이면 당뇨병을 가지고 있음을 나타냄을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>num_preg</th>\n",
       "      <th>PGC</th>\n",
       "      <th>DBP</th>\n",
       "      <th>tricept</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ped_func</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.34600</td>\n",
       "      <td>0.216471</td>\n",
       "      <td>0.610261</td>\n",
       "      <td>0.561934</td>\n",
       "      <td>0.311048</td>\n",
       "      <td>0.125230</td>\n",
       "      <td>0.475377</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.192167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.47617</td>\n",
       "      <td>0.192480</td>\n",
       "      <td>0.158489</td>\n",
       "      <td>0.166278</td>\n",
       "      <td>0.248020</td>\n",
       "      <td>0.170124</td>\n",
       "      <td>0.118355</td>\n",
       "      <td>0.144031</td>\n",
       "      <td>0.188961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.587940</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.122758</td>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.208750</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.246904</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label    num_preg         PGC         DBP     tricept     insulin  \\\n",
       "count  500.00000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
       "mean     0.34600    0.216471    0.610261    0.561934    0.311048    0.125230   \n",
       "std      0.47617    0.192480    0.158489    0.166278    0.248020    0.170124   \n",
       "min      0.00000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.00000    0.058824    0.497487    0.508197    0.000000    0.000000   \n",
       "50%      0.00000    0.176471    0.587940    0.590164    0.349206    0.056667   \n",
       "75%      1.00000    0.352941    0.708543    0.655738    0.507937    0.208750   \n",
       "max      1.00000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              BMI    ped_func         age  \n",
       "count  500.000000  500.000000  500.000000  \n",
       "mean     0.475377    0.172000    0.192167  \n",
       "std      0.118355    0.144031    0.188961  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.400522    0.072054    0.050000  \n",
       "50%      0.476900    0.122758    0.116667  \n",
       "75%      0.548435    0.246904    0.287500  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot statistical Infomation.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe객체의 describe 함수를 이용해서 training set의  데이터들에 대한 대략적인 통계 정보들을 확인하였다.\n",
    "문제에 나와있다 시피 training dataset의 data point 개수는 500개 였으며 각 feature들의 평균값 25%,50%,75% 값들을 통해 특정 데이터가 상대적으로 어느 위치에 있는지 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data.values  # Change dataframe object to numpy array\n",
    "test_data_np = test_data.values\n",
    "\n",
    "y = data_np[:,0] # Data preprocessing\n",
    "X = data_np[:,1:]\n",
    "y_t = test_data_np[:,0]\n",
    "X_t = test_data_np[:,1:]\n",
    "N = len(data['label'].values)\n",
    "N_t = len(test_data['label'].values)\n",
    "X2 = np.c_[np.ones(N), X] # Consider Intercept term \n",
    "X2_t = np.c_[np.ones(N_t), X_t]\n",
    "\n",
    "w_init = np.zeros(len(X[0,:]))\n",
    "\n",
    "\n",
    "def sigmoid(x): # Implementing sigmoid function\n",
    "\n",
    "        return 1/(1 + math.exp(-x))\n",
    "\n",
    "def sign(x): # Implementing Labeling function\n",
    "    if x>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def error_calculate(X, y, w): # Implementing error calculation function\n",
    "    y_p = np.zeros(X.shape[0])\n",
    "    sum_error=0\n",
    "    for i in range(X.shape[0]):\n",
    "        y_p[i] = sign(sigmoid(np.dot(X[i,:],w)))\n",
    "        error = y_p[i] -  y[i]\n",
    "        sum_error += error**2 \n",
    "    \n",
    "    return sum_error /X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터들을 처리하기 위해 넘파이 어레이 형태로 바꿔주었고, 종속변수와 독립변수 부분을 분리하였다. 또한 문제 1번과 2번을 해결하는 데에 필요한 함수들을 만들었다. sigmoid함수는 확률값을 계산하는 데에 사용하고, sign함수는 확률값이 0.5보다 크면 y를 1로 판정하고 그렇지 않으면 y를 0으로 판정하는 함수이다. 또한 error_calculate함수는 데이터셋과 w를 받아서 Average sum of squared에러를 계산해서 반환하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logis_sgd(X, y, X_t, y_t, w, etha, iterations):\n",
    "    \n",
    "    # Initialize value\n",
    "    whist=[]\n",
    "    error_history = []\n",
    "    rss =[]\n",
    "    p = np.zeros(len(w))\n",
    "    y_p = np.zeros(X.shape[0])\n",
    "    sum_error = 0\n",
    "    t=0\n",
    "    \n",
    "    while(t < iterations):\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            \n",
    "            m= y[i]-sigmoid(np.dot(X[i,:],w))\n",
    "            for j in range(len(w)): # Updating w\n",
    "            \n",
    "                p[j] = X[i,j]*m\n",
    "                w[j] = w[j] + (etha*p[j])\n",
    "                \n",
    "            \n",
    "            whist.append(w.copy())  # Saving w values\n",
    "            y_p[i] = sign(sigmoid(np.dot(X[i,:],w))) # Calculating error\n",
    "            error = y_p[i] -  y[i]\n",
    "            sum_error += error**2\n",
    "            \n",
    "            if t!=0 and t%100==0:\n",
    "                error_history.append(sum_error/t)\n",
    "                rss.append(error_calculate(X_t,y_t,w)) # Saving test error \n",
    "            \n",
    "            \n",
    "            t = t+1\n",
    "            \n",
    "\n",
    "    return  error_history,whist,rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient ascent for logistic regression을 위의 Logis_sgd 함수를 만들어서 구현하였다.\n",
    "기존 gradient ascent의 경우\n",
    "\\\\(\\frac { \\partial ll(w) }{ \\partial { w }_{ j } } =\\sum _{ i=1 }^{ N }{ \\frac { \\partial l{ l }_{ i }(w) }{ \\partial { w }_{ j } }  } \\quad \\\\) 이지만\n",
    "\n",
    "stochastic gradient ascent 의 경우\n",
    "\\\\(\\frac { \\partial ll(w) }{ \\partial { w }_{ j } } =\\frac { \\partial l{ l }_{ i }(w) }{ \\partial { w }_{ j } } \\\\) 이다.\n",
    "여기서 ll(w)는 log - likelihood function이다. \n",
    "iteration 마다 각각 다른 data point i를 사용하는 것이 특징이다. 여기서는 문제에서 데이터가 무작위로 정렬되있다고 하였으므로 순차적으로 해도 무관하기 때문에 그렇게 구현하였다. 또한 iteration 마다 w를 업데이트 하며 한 iteraion에서 데이터 포인트 하나를 가지고 w를 업데이트 한다. \n",
    "\n",
    "구체적으로는 j번째 feature value를 업데이트 할 때는 다음과 같은 과정을 거친다.\n",
    "\n",
    "\\\\({ w }_{ j }^{ (t+1) }={ w }_{ j }^{ (t) }+\\eta ({ h }_{ j }({ x }_{ i })(indic({ y }_{ i }=1)-P(y=1|{ x }_{ i },{ w }^{ (t) })))\\\\)\n",
    "\n",
    "여기서 t는 iteration 이며 indic()은 indicator function 이다. 에타는 learning rate값을 의미한다.  조건부 확률 값은 Sigmoid 함수에 Score value를 넣었을 때 나오는 함숫값으로 구한다.\n",
    "\n",
    "\n",
    "추가적으로 Logis_sgd에서는 문제 1.2와 1.3에서 iteration마다 error를 계산해야 해서 Logistic regression SGD를 구현함과 동시에 error를 계산해서 iteration 100번마다 error를 저장하게 하였고, 문제2.2에서 편하게 평균 test error를 계산하기 위해 test error도 iteration 100번마다 저장하였다. 또한 2번에서의 편의를 위해 iteration마다 갱신되는 w값들을 저장한 리스트인 whist를 반환하게 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "errhist,whist,rss = logis_sgd(X,y,X_t,y_t,w_init,0.8,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P1.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = range(0,len(errhist))\n",
    "y1 = [errhist[v] for v in x1]\n",
    "\n",
    "# Plot the graph\n",
    "plt.plot(x1,y1,'c-',lw=2.5)\n",
    "\n",
    "plt.ylabel('L value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러를 계산할 때는 각 데이터 포인트마다 예측된 라벨과 진짜 라벨값의 차이를 제곱한것들을 더해서 구한다. 예측되는 라벨값은 sigmoid함숫값이 0.5보다 크면 1, 0.5보다 작으면 0 으로 판정하는 sign함수를 활용하여 구하였다.\n",
    "100000번의 iteration을 수행하면서 100번마다 그때까지의 누적 평균 Loss를 저장해둔 Logis_sgd함수에서 반환한 errhist를 가지고 plot하였다.\n",
    "따라서 1000개의 값이 있고 이때의 x축 값들은 iteration 수가 아니라 iteration 수를 100으로 나눈 값이다.\n",
    "위의 표에서 보듯이 누적 평균 loss가 급격하게 감소함을 확인할 수 있고, iteration초기에는 진동폭이 크지만 iteration횟수가 증가할수록 진동폭이 감소함을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x3 = range(0,len(rss))\n",
    "y3 = [rss[v] for v in x3]\n",
    "\n",
    "# Plot the graph\n",
    "plt.plot(x3,y3,'c-',lw=2.5)\n",
    "\n",
    "plt.ylabel('Average sum of squared error value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 1.3에서는 training set에서 100,200,...번째 iteration 마다의 w값을 추출하여 이 w값을 이용하여 test data set에 대해 예측치를 구하고 이 예측치와 테스트 데이터의 참값과의 차이를 통해 average sum of squared error를 구했다. 이는 Logis_sgd 함수에서 iteration이 100번째마다의 w를 error_calculate함수의 인자로 넣어서 test data에 대한 error를 rss라는 리스트에 넣어서 반환하였다. \n",
    "이후 이를 plot한 위의 결과를 보면 iteration이 증가할수록 평균 sum of squared값이 급격하게 감소함을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P2.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(X,y,w):\n",
    "    cnt = 0\n",
    "    y_p = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        y_p[i] = sign(sigmoid(np.dot(X[i,:],w)))\n",
    "        if y[i] == y_p[i]:\n",
    "            cnt += 1\n",
    "    \n",
    "    return cnt/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = cal_accuracy(X_t,y_t,whist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5634328358208955"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1의 Logis_sgd함수를 보면 마지막에 iteration마다 저장한 w들을 반환함을 확인할 수 있다. 그래서 총 10만개의 w값이 저장된 리스트인 whist의 마지막 w값을 모델 정확도를 판별하는데 사용한다. 정확도는 전체 데이터 포인트 개수에서 예측치와 참값이 일치하는 데이터 포인트 개수의 비율로 정의되므로 가중치 w와 test 데이터셋의 독립변수값과 종속변수값들을 인자로 받는 cal_accuracy 함수를 구현하여 예측치와 참값이 같은 데이터 포인트의 개수를 세서 전체 데이터 포인트 개수로 나눈 accuracy를 반환하였다. 그 결과 테스트 정확도가 약 0.56이 나왔다. 이는 모델이 너무 단순하고 w값이 계속 진동하는 것 때문에 나타난 결과로 보인다. 또한 데이터셋에 대한 자세한 정보가 없지만 직접 데이터들을 살펴본 결과 데이터들의 feature value가 0인 부분이 군데군데 있었다. 나이 같은 0일 수가 없는 수치까지도 0인 항목들이 조금 있었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w(w_hist):\n",
    "    sum_w = np.zeros(8)\n",
    "    for i in range(99000,100000):\n",
    "        sum_w += whist[i]\n",
    "    return sum_w/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w = avg_w(whist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy2 = cal_accuracy(X_t,y_t,avg_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.667910447761194"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2에서는 총 십만번의 iteration 중 마지막 1000개의 가중치들의 평균을 사용하여 정확도를 측정한다. 십만개의w값들이 저장되어있는 whist를 인자로 받는 avg_w라는 함수를 구현하여 99000번째부터 99999번째까지의 가중치들의 평균 가중치를 반환하게 하였다. 이 평균 가중치를 사용하여 테스트 데이터셋에 대해 정확도를 측정한 결과 2.1에서의 결과인 56%보다 거의 10%나 향상된 약 67%의 정확도가 나타났다. 이 결과를 봤을 때, 가중치들의 진동폭의 영향이 어느 정도 있고, 이는 평균 가중치를 사용하면 영향을 상쇄시켜서 정확도를 눈에 띄게 향상시킬 수 있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight for BMI: -1.3059873054861209, weight for Insulin: 1.1216882849050351, weight for PGC: 2.1600140285727547 \n"
     ]
    }
   ],
   "source": [
    "print('weight for BMI: {0}, weight for Insulin: {1}, weight for PGC: {2} '.format(avg_w[5],avg_w[4],avg_w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다 정확한 accuracy를 보였던 문제2.2에서 계산한 평균 w값을 사용하였다. \n",
    "BMI와 Insulin, PGC에 대한 가중치를 살펴 본 결과, 세 항목 중 가장 당뇨병 발병 여부에 영향을 크게 끼치는 항목은 가장 절댓값이 높은 PGC라고 볼 수 있다. 또한 특정 항목에 해당하는 feature값의 부호가 '+'이고 크기가 클수록 score value가 커지고 score value가 커질수록 sigmoid fuction의 함숫값이 1에 가까워 지므로 당뇨병이라고 판정될 확률이 큰 것이다. 따라서 PGC 의 w는 약 2.16이므로 PGC가 클수록 당뇨병으로 판정될 확률이 높다고 생각할 수 있다. 반대로 weight 부호가 '-'일때는 그 반대 작용으로 정상이라고 판정될 확률이 커지는 것이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
